#!/usr/bin/env python3
"""
Yandex Maps GeliÅŸtirilmiÅŸ Yorum Scraper
Path: enhanced_yandex_scraper.py

Veri kalitesi iyileÅŸtirilmiÅŸ, tekrarlar azaltÄ±lmÄ±ÅŸ ve daha fazla veri elementi Ã§Ä±karan scraper
"""

import asyncio
import json
import os
import re
import hashlib
from playwright.async_api import async_playwright, TimeoutError
from datetime import datetime
import pandas as pd
import logging
import time

# KlasÃ¶rleri oluÅŸtur
os.makedirs('data/raw', exist_ok=True)
os.makedirs('data/processed', exist_ok=True)
os.makedirs('data/autosave', exist_ok=True)  # Otomatik kayÄ±tlar iÃ§in yeni klasÃ¶r

# Logging ayarlarÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scraper.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class YandexMapsScraper:
    def __init__(self):
        import collections
        self.base_url = "https://yandex.com.tr/maps"
        self.session_cookies = None
        self.page = None
        self.browser = None
        self.total_reviews = 0
        # Sadece son 30 yorumu kontrol etmek iÃ§in collections.deque kullan
        self.recent_review_ids = collections.deque(maxlen=30)
        self.recent_content_hashes = collections.deque(maxlen=30)
        self.duplicate_count = 0  # Tekrar sayÄ±sÄ±nÄ± izlet()
        # GeliÅŸmiÅŸ tekrar kontrolÃ¼ iÃ§in tÃ¼m yorumlar boyunca hash seti
        self.global_content_hashes = set()
        # Otomatik kaydetme iÃ§in deÄŸiÅŸkenler
        self.auto_save_interval = 50  # Her 50 yorumda bir otomatik kaydetme yapÄ±lacak
        self.last_auto_save_count = 0
        self.business_id = None
        self.business_name = None
            
    async def start_browser(self):
        """Browser'Ä± baÅŸlat ve session kur"""
        self.playwright = await async_playwright().start()
        
        # Browser'Ä± yÃ¼kle (headless=False olursa gÃ¶rÃ¼nÃ¼r olur)
        self.browser = await self.playwright.chromium.launch(
            headless=True,  # Performans iÃ§in headless modda Ã§alÄ±ÅŸtÄ±r
            args=[
                '--no-sandbox', 
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu'
            ]
        )
        
        context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080}
        )
        
        self.page = await context.new_page()
        
    async def navigate_to_place(self, business_url):
        """Yandex Maps'teki iÅŸletme sayfasÄ±na git"""
        logger.info(f"ğŸŒ Ä°ÅŸletme sayfasÄ±na yÃ¶nlendiriliyor: {business_url}")
        
        # Ana sayfaya git
        await self.page.goto(business_url)
        await self.page.wait_for_load_state('networkidle')
        
        # CAPTCHA kontrolÃ¼
        if await self.check_and_handle_captcha():
            logger.info("âœ… CAPTCHA iÅŸlemi tamamlandÄ±, devam ediliyor...")
            
        # Yorumlar sekmesine geÃ§
        if not await self.navigate_to_reviews_tab():
            logger.error("âŒ Yorumlar sekmesine geÃ§ilemedi!")
            return None, None
            
        # Ä°ÅŸletme bilgilerini topla
        business_name = await self.get_business_name()
        logger.info(f"ğŸ¢ Ä°ÅŸletme AdÄ±: {business_name}")
        
        # URL'den business ID Ã§Ä±kar
        business_id = self.extract_business_id(business_url)
        logger.info(f"ğŸ†” Business ID: {business_id}")
        
        # Toplam yorum sayÄ±sÄ±nÄ± al
        self.total_reviews = await self.get_total_review_count()
        logger.info(f"ğŸ“Š Toplam yorum sayÄ±sÄ±: {self.total_reviews}")
        
        return business_id, business_name
        
    async def check_and_handle_captcha(self):
        """CAPTCHA sayfasÄ±nÄ± kontrol et ve yÃ¶net"""
        current_url = self.page.url
        page_title = await self.page.title()
        
        is_captcha = ('showcaptcha' in current_url or 
                      'captcha' in current_url.lower() or 
                      'robot' in page_title.lower() or
                      'Are you not a robot' in page_title)
        
        if is_captcha:
            logger.warning("âš ï¸ CAPTCHA tespit edildi! LÃ¼tfen tarayÄ±cÄ±da CAPTCHA'yÄ± Ã§Ã¶zÃ¼n.")
            logger.warning("âš ï¸ CAPTCHA Ã§Ã¶zÃ¼ldÃ¼kten sonra entere basÄ±n...")
            
            # headless=True ise CAPTCHA iÃ§in tarayÄ±cÄ±yÄ± gÃ¶rÃ¼nÃ¼r yap
            if self.browser:
                await self.browser.close()
                
            logger.info("ğŸ”„ CAPTCHA Ã§Ã¶zÃ¼mÃ¼ iÃ§in gÃ¶rÃ¼nÃ¼r tarayÄ±cÄ± aÃ§Ä±lÄ±yor...")
            self.browser = await self.playwright.chromium.launch(headless=False)
            context = await self.browser.new_context()
            self.page = await context.new_page()
            
            # CAPTCHA sayfasÄ±na git
            await self.page.goto(current_url)
            
            input("CAPTCHA Ã§Ã¶zÃ¼ldÃ¼ÄŸÃ¼nde Enter tuÅŸuna basÄ±n...")
            
            # SayfanÄ±n yÃ¼klenmesini bekle
            await self.page.wait_for_load_state('networkidle')
            return True
            
        return False
    
    def extract_business_id(self, url):
        """URL'den business ID'yi Ã§Ä±karÄ±r"""
        match = re.search(r'/org/[^/]+/(\d+)', url)
        if match:
            return match.group(1)
        return None
    
    async def get_business_name(self):
        """Sayfa baÅŸlÄ±ÄŸÄ±ndan iÅŸletme adÄ±nÄ± al"""
        try:
            title = await self.page.title()
            # "Ä°ÅŸletme AdÄ± â€” Yandex Haritalar" formatÄ±ndan iÅŸletme adÄ±nÄ± Ã§Ä±kar
            if " â€” " in title:
                return title.split(" â€” ")[0]
            elif "robot" in title.lower() or "captcha" in title.lower():
                return "CAPTCHA SayfasÄ±"
            return title
        except:
            return "Bilinmeyen Ä°ÅŸletme"
    
    async def get_total_review_count(self):
        """Toplam yorum sayÄ±sÄ±nÄ± sayfadan Ã§Ä±kar"""
        try:
            # Yorum sayÄ±sÄ±nÄ± iÃ§eren metni bul
            review_count_text = await self.page.evaluate("""
                () => {
                    {
                    // 0. H2 Ã¶zel baÅŸlÄ±k
                    const h2Review = document.querySelector('h2.card-section-header__title._wide');
                    if (h2Review) {
                        const text = h2Review.textContent || '';
                        const match = text.match(/(\d[\d\s,.]*\d+)/);
                        if (match) return match[0];
                    }                                     
                    
                    // 1. Yorumlar sekmesi Ã¼zerindeki sayÄ±
                    const reviewTab = document.querySelector('[data-tab-name="reviews"], [role="tab"]:has-text("Yorumlar"), [role="tab"]:has-text("Reviews"), [role="tab"]:has-text("ĞÑ‚Ğ·Ñ‹Ğ²Ñ‹")');
                    if (reviewTab) {
                        const text = reviewTab.textContent || '';
                        const match = text.match(/(\\d[\\d\\s,.]*\\d+)/);
                        if (match) return match[0];
                    }
                    
                    // 2. Yorumlar baÅŸlÄ±ÄŸÄ±ndaki sayÄ±
                    const headers = document.querySelectorAll('h1, h2, h3, .header, .title');
                    for (const header of headers) {
                        const text = header.textContent || '';
                        const match = text.match(/(\\d[\\d\\s,.]*\\d+)\\s*(yorum|Ğ¾Ñ‚Ğ·Ñ‹Ğ²|reviews)/i);
                        if (match) return match[1];
                    }
                    
                    // 3. Sayfa Ã¼zerinde herhangi bir yerdeki yorum sayÄ±sÄ±
                    const elements = document.querySelectorAll('*');
                    for (const elem of elements) {
                        const text = elem.textContent || '';
                        const match = text.match(/(\\d[\\d\\s,.]*\\d+)\\s*(yorum|Ğ¾Ñ‚Ğ·Ñ‹Ğ²|reviews)/i);
                        if (match) return match[1];
                    }
                    
                    return null;
                }
            """)
            
            if review_count_text:
                # SayÄ±yÄ± temizle (boÅŸluklar, noktalama iÅŸaretleri vs.)
                count_str = re.sub(r'\D', '', review_count_text)
                if count_str.isdigit():
                    return int(count_str)
            
            logger.warning("âš ï¸ Toplam yorum sayÄ±sÄ± belirlenemedi, varsayÄ±lan deÄŸer kullanÄ±lacak")
            return 1000  # VarsayÄ±lan bir deÄŸer
            
        except Exception as e:
            logger.error(f"âŒ Yorum sayÄ±sÄ± alÄ±nÄ±rken hata: {e}")
            return 1000  # Hata durumunda varsayÄ±lan bir deÄŸer
    
    async def navigate_to_reviews_tab(self):
        """Yorumlar sekmesine git"""
        logger.info("ğŸ” Yorumlar sekmesine geÃ§iliyor...")
        
        try:
            # 1. Yorumlar sekmesini ara
            tabs = await self.page.query_selector_all('[role="tab"], [data-tab-name], li[class*="tab"]')
            
            for tab in tabs:
                text = await tab.text_content()
                if re.search(r'(yorumlar|reviews|Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹|comments)', text, re.IGNORECASE):
                    logger.info(f"âœ… Yorumlar sekmesi bulundu: '{text}'")
                    await tab.click()
                    await asyncio.sleep(2)
                    return True
            
            # 2. XPath ile ara
            review_tab_xpath = "//a[contains(text(), 'Yorumlar')] | //a[contains(text(), 'Reviews')] | //a[contains(text(), 'ĞÑ‚Ğ·Ñ‹Ğ²Ñ‹')] | //div[contains(text(), 'Yorumlar')] | //div[contains(text(), 'Reviews')] | //div[contains(text(), 'ĞÑ‚Ğ·Ñ‹Ğ²Ñ‹')]"
            review_tab = await self.page.query_selector(review_tab_xpath)
            
            if review_tab:
                logger.info("âœ… Yorumlar sekmesi bulundu (XPath ile)")
                await review_tab.click()
                await asyncio.sleep(2)
                return True
                
            # 3. URL'de reviews kelimesi varsa zaten doÄŸru sekmedeyiz
            current_url = self.page.url
            if 'reviews' in current_url or 'yorumlar' in current_url:
                logger.info("âœ… Zaten yorumlar sekmesindeyiz")
                return True
            
            # 4. KullanÄ±cÄ±dan manuel geÃ§iÅŸ iste
            logger.warning("âš ï¸ Yorumlar sekmesi otomatik olarak bulunamadÄ±.")
            logger.warning("â„¹ï¸ LÃ¼tfen tarayÄ±cÄ±da 'Yorumlar' sekmesine manuel olarak tÄ±klayÄ±n")
            input("Yorumlar sekmesine geÃ§tikten sonra Enter tuÅŸuna basÄ±n...")
            await asyncio.sleep(2)
            return True
            
        except Exception as e:
            logger.error(f"âŒ Yorumlar sekmesine geÃ§erken hata: {e}")
            return False
    
    async def expand_review_texts(self):
        """Yorumlardaki 'DiÄŸer' butonlarÄ±na tÄ±klayarak uzun yorumlarÄ± tamamen geniÅŸletir (tÃ¼m butonlar bitene kadar)."""
        try:
            more_button_selectors = [
                "span.business-review-view__expand",
                "span.spoiler-view__button",
                "[aria-label='DiÄŸer']",
                "[aria-label='More']",
                "[aria-label='Ğ•Ñ‰Ñ‘']",
                "button:has-text('DiÄŸer')",
                "button:has-text('More')",
                "button:has-text('Daha fazla')",
                "[role='button']:has-text('DiÄŸer')",
                "[role='button']:has-text('More')"
            ]
            total_expanded = 0
            max_loops = 10  # Sonsuz dÃ¶ngÃ¼ye girmemek iÃ§in limit
            for _ in range(max_loops):
                expanded_this_round = 0
                for selector in more_button_selectors:
                    more_buttons = await self.page.query_selector_all(selector)
                    for button in more_buttons:
                        try:
                            is_visible = await button.is_visible()
                            if is_visible:
                                await button.click()
                                expanded_this_round += 1
                                await asyncio.sleep(0.2)
                        except Exception:
                            continue
                total_expanded += expanded_this_round
                if expanded_this_round == 0:
                    break  # ArtÄ±k aÃ§Ä±lacak buton kalmadÄ±
                await asyncio.sleep(0.3)
            if total_expanded > 0:
                logger.info(f"âœ… {total_expanded} adet 'DiÄŸer' butonu tÄ±klandÄ±, tÃ¼m uzun yorumlar geniÅŸletildi")
            return total_expanded
        except Exception as e:
            logger.error(f"âŒ Yorum geniÅŸletme iÅŸlemi sÄ±rasÄ±nda hata: {e}")
            return 0

    async def auto_save_reviews(self, all_reviews):
        """Belirli aralÄ±klarla review'larÄ± otomatik kaydet"""
        if len(all_reviews) == 0 or not self.business_id:
            return
            
        if len(all_reviews) - self.last_auto_save_count >= self.auto_save_interval:
            try:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # Otomatik kaydedilen verileri hazÄ±rla
                data = {
                    'business_id': self.business_id,
                    'business_name': self.business_name,
                    'reviews': all_reviews,
                    'total_review_count': self.total_reviews,
                    'scraped_review_count': len(all_reviews),
                    'scrape_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'auto_save': True
                }
                
                # JSON dosyasÄ± olarak kaydet
                autosave_filename = f"data/autosave/yandex_reviews_{self.business_id}_autosave_{timestamp}.json"
                with open(autosave_filename, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                # CSV dosyasÄ± olarak kaydet
                df = pd.DataFrame(all_reviews)
                csv_filename = f"data/autosave/yandex_reviews_{self.business_id}_autosave_{timestamp}.csv"
                df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
                
                logger.info(f"ğŸ’¾ Otomatik kayÄ±t: {len(all_reviews)} yorum kaydedildi (her {self.auto_save_interval} yorumda bir)")
                
                # Son kayÄ±t sayÄ±sÄ±nÄ± gÃ¼ncelle
                self.last_auto_save_count = len(all_reviews)
                
            except Exception as e:
                logger.error(f"âŒ Otomatik kayÄ±t sÄ±rasÄ±nda hata: {e}")

    async def scrape_reviews_with_continuous_scroll(self, max_reviews=None):
        """SÃ¼rekli kaydÄ±rma ile yorumlarÄ± Ã§ek (daha fazla scroll ve daha agresif 'DiÄŸer' aÃ§ma ile)."""
        if max_reviews is None:
            max_reviews = self.total_reviews

        logger.info(f"ğŸ” Yorumlar Ã§ekiliyor (hedef: {max_reviews})...")

        all_reviews = []
        last_height = 0
        no_new_content_count = 0

        best_selector = await self.find_best_review_selector()
        if not best_selector:
            logger.error("âŒ HiÃ§bir yorum elementi bulunamadÄ±!")
            return []

        logger.info(f"âœ… En uygun selektÃ¶r: {best_selector}")

        page_size_estimate = 15
        scroll_count = max(5, min(max_reviews // page_size_estimate, 20))  # Daha fazla scroll
        max_attempts = min(max_reviews // 5, 300)  # Daha fazla deneme

        attempts = 0
        while len(all_reviews) < max_reviews and attempts < max_attempts:
            # Scroll Ã¶ncesi ve sonrasÄ± agresif ÅŸekilde tÃ¼m 'DiÄŸer' butonlarÄ±nÄ± aÃ§
            await self.expand_review_texts()

            for _ in range(scroll_count):
                await self.try_multiple_scroll_methods()
                await asyncio.sleep(1.5)
                await self.expand_review_texts()

            # Scroll sonrasÄ± tekrar tÃ¼m 'DiÄŸer' butonlarÄ±nÄ± aÃ§
            await self.expand_review_texts()

            current_height = await self.page.evaluate('document.body.scrollHeight')
            elements = await self.page.query_selector_all(best_selector)
            current_element_count = len(elements)

            logger.info(f"ğŸ“œ Åu ana kadar bulunan yorum sayÄ±sÄ±: {current_element_count}")

            start_index = len(all_reviews)
            if start_index < current_element_count:
                logger.info(f"âœ¨ {current_element_count - start_index} yeni yorum bulundu")
                for i in range(start_index, current_element_count):
                    try:
                        review_data = await self.extract_review_data(elements[i])
                        if review_data and self.is_valid_review(review_data):
                            if not self.is_duplicate_review(review_data):
                                all_reviews.append(review_data)
                                if len(all_reviews) % 25 == 0:
                                    logger.info(f"âœ… {len(all_reviews)} yorum iÅŸlendi")
                                await self.auto_save_reviews(all_reviews)
                    except Exception as e:
                        logger.error(f"âŒ Yorum Ã§Ä±karma hatasÄ±: {e}")
                no_new_content_count = 0
            else:
                no_new_content_count += 1
                logger.info(f"âš ï¸ Yeni yorum yÃ¼klenmedi. Deneme: {no_new_content_count}/3")

            if no_new_content_count >= 3 or (current_height == last_height and no_new_content_count >= 2):
                logger.info("ğŸ”„ Alternatif kaydÄ±rma yÃ¶ntemleri deneniyor...")
                await self.try_alternative_loading_methods()
                await asyncio.sleep(3)
                await self.expand_review_texts()
                new_elements = await self.page.query_selector_all(best_selector)
                if len(new_elements) <= current_element_count:
                    logger.info("âš ï¸ Daha fazla yorum yÃ¼klenemedi, mevcut yorumlarla devam ediliyor")
                    break

            last_height = current_height

            if len(all_reviews) >= max_reviews:
                logger.info(f"ğŸ¯ Hedef yorum sayÄ±sÄ±na ulaÅŸÄ±ldÄ±: {max_reviews}")
                break

            attempts += 1

        logger.info(f"ğŸ‰ Toplam {len(all_reviews)} yorum baÅŸarÄ±yla Ã§ekildi!")
        return all_reviews
    
    async def find_best_review_selector(self):
        """Sayfadaki en iyi yorum selektÃ¶rÃ¼nÃ¼ bul"""
        review_selectors = [
            "div.spoiler-view__text span.spoiler-view__text-container",
            "div[class*='business-reviews-card']",
            "div[class*='review']",
            "[class*='review-item']",
            "li[class*='card']",
            "[class*='comment']",
            "div[class*='feed-item']",
            "div[class*='_card_']",
            "div[class*='rating-']"
        ]
        
        best_selector = None
        max_count = 0
        
        for selector in review_selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                if len(elements) > max_count:
                    # Ä°lk elementi kontrol et - gerÃ§ekten yorum mu?
                    if len(elements) > 0:
                        element_text = await elements[0].text_content()
                        # Yorum benzeri metin iÃ§eriyor mu?
                        if re.search(r'(star|puan|rating|yorum|review|Ğ¾Ñ‚Ğ·Ñ‹Ğ²)', element_text, re.IGNORECASE):
                            max_count = len(elements)
                            best_selector = selector
                            logger.info(f"Potansiyel selektÃ¶r: '{selector}' ile {len(elements)} element bulundu")
            except Exception as e:
                logger.debug(f"SelektÃ¶r '{selector}' hatasÄ±: {e}")
        
        # EÄŸer hiÃ§bir selektÃ¶r bulunamadÄ±ysa, sayfa HTML'ini analiz et
        if not best_selector:
            logger.info("ğŸ” Sayfa yapÄ±sÄ± analiz ediliyor...")
            
            # Sayfa HTML'inden class isimlerini Ã§Ä±kart
            page_html = await self.page.content()
            
            # Yorum ile ilgili class isimlerini ara
            review_classes = []
            for keyword in ['review', 'comment', 'feed', 'card', 'rating', 'Ğ¾Ñ‚Ğ·Ñ‹Ğ²', 'yorum']:
                pattern = f'class="([^"]*{keyword}[^"]*)"'
                matches = re.findall(pattern, page_html, re.IGNORECASE)
                review_classes.extend(matches[:5])  # Her keyword iÃ§in en fazla 5 match
            
            # En fazla 10 sÄ±nÄ±f dene
            for class_name in review_classes[:10]:
                selector = f'[class*="{class_name}"]'
                elements = await self.page.query_selector_all(selector)
                if len(elements) > max_count:
                    max_count = len(elements)
                    best_selector = selector
                    logger.info(f"Bulunan selektÃ¶r: '{selector}' ile {len(elements)} element bulundu")
        
        return best_selector
    
    async def try_multiple_scroll_methods(self):
        """FarklÄ± kaydÄ±rma yÃ¶ntemlerini dene"""
        try:
            # 1. JavaScript ile sayfayÄ± kaydÄ±r
            await self.page.evaluate("window.scrollBy(0, 500)")
            await asyncio.sleep(0.5)
            
            # 2. End tuÅŸu ile sayfa sonuna git
            await self.page.keyboard.press("End")
            await asyncio.sleep(0.5)
            
            # 3. PageDown tuÅŸu ile aÅŸaÄŸÄ± in
            await self.page.keyboard.press("PageDown")
            
        except Exception as e:
            logger.debug(f"KaydÄ±rma hatasÄ±: {e}")
    
    async def try_alternative_loading_methods(self):
        """Alternatif iÃ§erik yÃ¼kleme yÃ¶ntemlerini dene"""
        try:
            # 1. "Daha fazla gÃ¶ster" butonlarÄ±nÄ± ara
            load_more_selectors = [
                "button:has-text('Daha fazla')",
                "button:has-text('Load more')",
                "button:has-text('Show more')",
                "button:has-text('Ğ•Ñ‰Ğµ')",
                "button:has-text('ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ĞµÑ‰Ğµ')",
                "[class*='show-more']",
                "[class*='load-more']"
            ]
            
            for selector in load_more_selectors:
                try:
                    load_more = await self.page.query_selector(selector)
                    if load_more:
                        logger.info(f"âœ… 'Daha fazla' butonu bulundu: {selector}")
                        await load_more.click()
                        await asyncio.sleep(2)
                        return True
                except:
                    continue
            
            # 2. JavaScript ile sayfayÄ± yenile ve scroll event'i tetikle
            await self.page.evaluate("""
                () => {
                    window.dispatchEvent(new Event('scroll'));
                    window.scrollTo(0, document.body.scrollHeight);
                }
            """)
            await asyncio.sleep(2)
            
            # 3. FarklÄ± kaydÄ±rma teknikleri
            for scroll_pos in [1000, 2000, 3000, 5000]:
                await self.page.evaluate(f"window.scrollTo(0, {scroll_pos})")
                await asyncio.sleep(0.5)
            
            # 4. Space tuÅŸu ile kaydÄ±r
            for _ in range(5):
                await self.page.keyboard.press("Space")
                await asyncio.sleep(0.5)
                
            return True
            
        except Exception as e:
            logger.error(f"âŒ Alternatif yÃ¼kleme yÃ¶ntemleri hatasÄ±: {e}")
            return False
    
    def is_valid_review(self, review_data):
        """Yorumun geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        # Minimum metin uzunluÄŸu kontrolÃ¼
        if 'text_original' not in review_data or not review_data['text_original']:
            return False
            
        # Ã‡ok kÄ±sa yorumlarÄ± filtrele 
        if len(review_data['text_original']) <= 1:
            return False
            
        # Yazar adÄ± kontrolÃ¼
        if 'author_name' not in review_data or not review_data['author_name']:
            return False
            
        return True
    
    def is_duplicate_review(self, review_data):
        """Bir yorumu hem son 30 yorumda hem de tÃ¼m veri boyunca normalize edilmiÅŸ metin hash'iyle tekrar kontrol eder"""
        # Review ID'yi kontrol et (kÄ±sa vadeli tekrarlar iÃ§in)
        if 'review_id' in review_data and review_data['review_id'] in self.recent_review_ids:
            self.duplicate_count += 1
            return True

        # GeliÅŸmiÅŸ tekrar kontrolÃ¼: normalize edilmiÅŸ metin hash'i
        text = review_data.get('text_original', '')
        norm_text = self.normalize_review_text(text)
        content_hash = hashlib.md5(norm_text.encode()).hexdigest()

        # TÃ¼m veri boyunca tekrar kontrolÃ¼
        if content_hash in self.global_content_hashes:
            self.duplicate_count += 1
            return True

        # Son 30 iÃ§in de tutmaya devam et
        if 'review_id' in review_data:
            self.recent_review_ids.append(review_data['review_id'])
        self.recent_content_hashes.append(content_hash)
        self.global_content_hashes.add(content_hash)

        return False

    def normalize_review_text(self, text):
        """Yorum metnini normalize ederek tekrar kontrolÃ¼nÃ¼ iyileÅŸtirir (kÃ¼Ã§Ã¼k harf, noktalama, gereksiz boÅŸluk, baÅŸtaki/sondaki tarih/seviye/isim temizliÄŸi)."""
        import re
        if not text:
            return ''
        # KÃ¼Ã§Ã¼k harfe Ã§evir
        text = text.lower()
        # BaÅŸta/sonda tarih, seviye, isim gibi tekrar eden yapÄ±larÄ± temizle
        text = re.sub(r'^[^a-zA-Z0-9Ğ°-ÑĞ-Ğ¯Ã§ÄŸÄ±Ã¶ÅŸÃ¼]+', '', text)
        text = re.sub(r'\d+\.\s*ÅŸehir uzmanÄ±', '', text)
        text = re.sub(r'\d+\.\s*level local guide', '', text)
        text = re.sub(r'\d+\s*(temmuz|nisan|mart|ocak|ÅŸubat|mayÄ±s|haziran|temmuz|aÄŸustos|eylÃ¼l|ekim|kasÄ±m|aralÄ±k|january|february|march|april|may|june|july|august|september|october|november|december)\s*', '', text)
        text = re.sub(r'\d{1,2}\.\d{1,2}\.\d{4}', '', text)
        text = re.sub(r'\d{1,2}/\d{1,2}/\d{4}', '', text)
        # Noktalama ve fazla boÅŸluklarÄ± kaldÄ±r
        text = re.sub(r'[\W_]+', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    async def extract_review_data(self, review_element):
        """Bir yorum elementinden veri Ã§Ä±kar - GELÄ°ÅTÄ°RÄ°LMÄ°Å VERSÄ°YON"""
        try:
            # Element HTML ve metnini al (debug iÃ§in)
            element_html = await self.page.evaluate('(element) => element.outerHTML', review_element)
            element_text = await review_element.text_content()
            
            # ---- YAZAR ADI ----
            author_name = await self.extract_author_name(review_element, element_text)
            
            # ---- PUAN ----
            rating = await self.extract_rating(review_element, element_html)
            
            # ---- YORUM METNÄ° ----
            text = await self.extract_text_content(review_element, element_text, author_name)
            
            # ---- TARÄ°H ----
            date = await self.extract_date(review_element)
            
            # ---- FOTOÄRAFLAR ----
            has_photos = await self.has_photos(review_element)
            
            # ---- Ä°ÅLETME YANITI ----
            business_reply = await self.extract_business_reply(review_element)
            
            # ID oluÅŸtur
            review_id = hashlib.md5(f"{author_name}_{text}_{date}".encode()).hexdigest()
            
            return {
                'review_id': review_id,
                'author_name': author_name,
                'rating': rating,
                'text_original': text,
                'date': date,
                'has_photos': has_photos,
                'business_reply': business_reply
            }
            
        except Exception as e:
            logger.error(f"âŒ Yorum veri Ã§Ä±karma hatasÄ±: {e}")
            return None
    
    async def extract_author_name(self, review_element, element_text=None):
        """Yorum elementinden yazar adÄ±nÄ± Ã§Ä±kar"""
        author_name = "Anonim"
        
        # AdÄ±m 1: CSS seÃ§icilerle ara
        author_selectors = [
            "[class*='user']", 
            "[class*='author']",
            "[class*='name']",
            "span[itemprop='name']",
            "a[href*='profile']",
            "[class*='profile']"
        ]
        
        for selector in author_selectors:
            try:
                author_element = await review_element.query_selector(selector)
                if author_element:
                    author_text = await author_element.text_content()
                    author_text = author_text.strip()
                    if author_text and len(author_text) > 0 and len(author_text) < 100:
                        # YaygÄ±n gereksiz metinleri temizle
                        author_text = re.sub(r'(Abone ol|Subscribe|Follow|seviye|level|ÑƒĞ·Ğ½Ğ°Ñ‚ÑŒ|ÑĞºÑĞ¿ĞµÑ€Ñ‚)', '', author_text, flags=re.IGNORECASE)
                        author_text = re.sub(r'\s+', ' ', author_text).strip()  # Fazla boÅŸluklarÄ± temizle
                        
                        if author_text:  # Temizlemeden sonra hala metin varsa
                            author_name = author_text
                            break
            except:
                pass
        
        # AdÄ±m 2: Tipik yazar adÄ± desenlerini regex ile ara
        if author_name == "Anonim" and element_text:
            author_patterns = [
                r'([A-Za-zĞ-Ğ¯Ğ°-ÑÃ‡Ã§ÄÄŸÄ°Ä±Ã–Ã¶ÅÅŸÃœÃ¼]{2,}\s+[A-Za-zĞ-Ğ¯Ğ°-ÑÃ‡Ã§ÄÄŸÄ°Ä±Ã–Ã¶ÅÅŸÃœÃ¼]{2,})\s+(\d+\.|\d+\s+seviye|level)',  # "John Smith 5. seviye"
                r'([A-Za-zĞ-Ğ¯Ğ°-ÑÃ‡Ã§ÄÄŸÄ°Ä±Ã–Ã¶ÅÅŸÃœÃ¼]{2,}\s+[A-Za-zĞ-Ğ¯Ğ°-ÑÃ‡Ã§ÄÄŸÄ°Ä±Ã–Ã¶ÅÅŸÃœÃ¼\.]{1,})\s+',  # "John Smith "
                r'^([A-Za-zĞ-Ğ¯Ğ°-ÑÃ‡Ã§ÄÄŸÄ°Ä±Ã–Ã¶ÅÅŸÃœÃ¼]{2,}\s+[A-Za-zĞ-Ğ¯Ğ°-ÑÃ‡Ã§ÄÄŸÄ°Ä±Ã–Ã¶ÅÅŸÃœÃ¼\.]{1,})'  # "John Smith" (baÅŸlangÄ±Ã§ta)
            ]
            
            for pattern in author_patterns:
                match = re.search(pattern, element_text)
                if match:
                    author_name = match.group(1).strip()
                    break
        
        return author_name
    
    async def extract_rating(self, review_element, element_html=None):
        """Yorum elementinden puanÄ± Ã§Ä±kar - gÃ¼ncel ve Ã§ok dilli"""
        rating = None

        # --- AdÄ±m 1: CSS seÃ§icilerle ara ---
        rating_selectors = [
            "[class*='rating']",
            "[class*='score']",
            "[class*='stars']",
            "[class*='star']",
            "meta[itemprop='ratingValue']",
            # Senin verdiÄŸin Ã¶zel Ã¶rnek class
            "div.business-rating-badge-view__stars"
        ]

        for selector in rating_selectors:
            try:
                rating_element = await review_element.query_selector(selector)
                if rating_element:
                    rating_text = await rating_element.get_attribute("aria-label") or await rating_element.text_content()
                    rating_match = re.search(r'(\d+(\.\d+)?)', rating_text)
                    if rating_match:
                        rating_value = float(rating_match.group(1))
                        if 0 <= rating_value <= 5:
                            rating = rating_value
                            break
            except:
                continue

        # --- AdÄ±m 2: HTML'de yÄ±ldÄ±z sayÄ±sÄ±nÄ± kontrol et ---
        if rating is None and element_html:
            star_count = element_html.count('â˜…')
            if 0 < star_count <= 5:
                rating = float(star_count)
            else:
                # SVG veya aria-label Ã¼zerinden de dene
                aria_match = re.search(r'(DeÄŸerlendirme|Rating|ĞÑ†ĞµĞ½ĞºĞ°)\s*(\d+)\s*/\s*5', element_html, re.IGNORECASE)
                if aria_match:
                    rating = float(aria_match.group(2))

        # --- AdÄ±m 3: Element metninde puan desenlerini ara ---
        if rating is None:
            element_text = await review_element.text_content()
            rating_patterns = [
                r'(\d+(\.\d+)?)\s*(?:out of|\/)\s*5',  # "4.5 out of 5" veya "4.5/5"
                r'rating\s*:?\s*(\d+(\.\d+)?)',       # "rating: 4.5" veya "Rating 4.5"
                r'(\d+(\.\d+)?)\s*stars?'             # "4.5 stars" veya "4.5 star"
            ]
            for pattern in rating_patterns:
                match = re.search(pattern, element_text, re.IGNORECASE)
                if match:
                    try:
                        rating_value = float(match.group(1))
                        if 0 <= rating_value <= 5:
                            rating = rating_value
                            break
                    except:
                        continue

        return rating

    
    async def extract_text_content(self, review_element, element_text=None, author_name=None):
        """Yorum elementinden metin iÃ§eriÄŸini Ã§Ä±kar"""
        text = ""
        
        # AdÄ±m 1: CSS seÃ§icilerle ara
        text_selectors = [
            "[class*='text']", 
            "[class*='content']", 
            "[class*='body']",
            "p", 
            "[class*='comment']",
            "[class*='message']",
            "span.spoiler-view__text-container",        # spoiler metinleri
            "[class*='description']",
            "div.spoiler-view__text._collapsed",       # collapse edilmiÅŸ div
            "div.business-review-view__text",          # yorum div'inin alternatif sÄ±nÄ±fÄ±
            "span.business-review-view__expand"        # geniÅŸletme butonlarÄ±yla birlikte iÃ§erik
        ]
        
        for selector in text_selectors:
            try:
                text_element = await review_element.query_selector(selector)
                if text_element:
                    text_content = await text_element.text_content()
                    text_content = text_content.strip()
                    # En az 5 karakter ve yazar adÄ±ndan farklÄ± olmalÄ±
                    if len(text_content) > 5 and (not author_name or text_content != author_name):
                        text = text_content
                        break
            except:
                pass
        
        # AdÄ±m 2: Metin bulunamadÄ±ysa, tÃ¼m iÃ§eriÄŸi al ve temizle
        if not text or text == "VarsayÄ±lan" or len(text) < 10:
            if element_text:
                # Yazar adÄ±nÄ± ve yaygÄ±n metinleri Ã§Ä±kar
                full_text = element_text
                
                # Gereksiz metinleri Ã§Ä±kar
                if author_name:
                    full_text = full_text.replace(author_name, "")
                
                common_texts = [
                    "VarsayÄ±lan", "Default", "Abone ol", "Subscribe", "Follow",
                    "seviye ÅŸehir uzmanÄ±", "level local guide", "yerel rehber",
                    "YanÄ±tla", "Reply", "BeÄŸen", "Like", "Share", "PaylaÅŸ"
                ]
                
                for common in common_texts:
                    full_text = full_text.replace(common, "")
                
                # Fazla boÅŸluklarÄ± temizle
                text = re.sub(r'\s+', ' ', full_text).strip()
                
                # Ã‡ok uzun metinleri kÄ±salt (veri kalitesini artÄ±rmak iÃ§in)
                if len(text) > 2000:
                    text = text[:2000] + "..."
        
        # AdÄ±m 3: Ä°lave temizleme
        if text:
            # BaÅŸta ve sonda tek karakterleri temizle
            text = re.sub(r'^[^a-zA-Z0-9Ğ°-ÑĞ-Ğ¯Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ]+', '', text)
            text = re.sub(r'[^a-zA-Z0-9Ğ°-ÑĞ-Ğ¯Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ]+$', '', text)
            
            # Ä°ÅŸletme yanÄ±tÄ±nÄ± Ã§Ä±kar (genellikle "Ä°ÅŸletme yanÄ±tÄ±: ..." formatÄ±nda olur)
            text = re.sub(r'Ä°ÅŸletme[^:]*:[^\n]*\n.*$', '', text, flags=re.DOTALL)
            text = re.sub(r'Business[^:]*:[^\n]*\n.*$', '', text, flags=re.DOTALL)
            text = re.sub(r'Owner[^:]*:[^\n]*\n.*$', '', text, flags=re.DOTALL)
        
        return text
    
    async def extract_date(self, review_element):
        """Yorum elementinden tarihi Ã§Ä±kar"""
        date = None
        
        # AdÄ±m 1: CSS seÃ§icilerle ara
        date_selectors = [
            "[class*='date']", 
            "[class*='time']", 
            "time", 
            ".business-review-view__date",  # yeni eklendi
            "[class*='when']",
            "[class*='posted']",
            "meta[itemprop='datePublished']",     # meta etiketi ile tarih
            "span[aria-label*='DeÄŸerlendirme']"   # bazÄ± sitelerde yÄ±ldÄ±z + tarih bilgisi aynÄ± span iÃ§inde olabilir
        ]
        
        for selector in date_selectors:
            try:
                date_element = await review_element.query_selector(selector)
                if date_element:
                    date_content = await date_element.text_content()
                    date_content = date_content.strip()
                    if date_content and len(date_content) > 2 and len(date_content) < 50:
                        # Tarih formatÄ±nÄ± daha da doÄŸrula
                        if re.search(r'\d', date_content):  # En azÄ±ndan bir rakam iÃ§ermeli
                            date = date_content
                            break
            except:
                pass
        
        # AdÄ±m 2: Metnin iÃ§inde tarih formatÄ±nÄ± ara
        if not date:
            element_text = await review_element.text_content()
            date_patterns = [
                r'(\d{1,2}\s+[A-Za-zĞ-Ğ¯Ğ°-ÑÃ‡Ã§ÄÄŸÄ°Ä±Ã–Ã¶ÅÅŸÃœÃ¼]+\s+\d{4})',  # "15 Ocak 2023"
                r'(\d{1,2}\s+[A-Za-zĞ-Ğ¯Ğ°-ÑÃ‡Ã§ÄÄŸÄ°Ä±Ã–Ã¶ÅÅŸÃœÃ¼]+)',  # "15 Ocak"
                r'([A-Za-zĞ-Ğ¯Ğ°-ÑÃ‡Ã§ÄÄŸÄ°Ä±Ã–Ã¶ÅÅŸÃœÃ¼]+\s+\d{1,2},\s*\d{4})',  # "Ocak 15, 2023"
                r'(\d{1,2}\.\d{1,2}\.\d{4})',  # "15.01.2023"
                r'(\d{1,2}/\d{1,2}/\d{4})'  # "15/01/2023"
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, element_text)
                if match:
                    date = match.group(1)
                    break
        
        return date
    
    async def has_photos(self, review_element):
        """Yorum elementinde fotoÄŸraf olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        # FotoÄŸraf gÃ¶stergeleri
        photo_selectors = [
            "img[src*='review']",
            "img[class*='photo']", 
            "[class*='gallery']",
            "[class*='photo']",
            "[class*='image']",
            "[class*='media']"
        ]
        
        for selector in photo_selectors:
            try:
                photos = await review_element.query_selector_all(selector)
                if len(photos) > 0:
                    return True
            except:
                pass
        
        # HTML iÃ§inde fotoÄŸraf gÃ¶stergeleri ara
        element_html = await self.page.evaluate('(element) => element.outerHTML', review_element)
        
        # Ä°Ã§erikte fotoÄŸraf referansÄ± var mÄ±?
        if re.search(r'(photo|image|picture|gallery|Ñ„Ğ¾Ñ‚Ğ¾|resim)', element_html, re.IGNORECASE):
            return True
            
        return False
    
    async def extract_business_reply(self, review_element):
        """Ä°ÅŸletme yanÄ±tÄ±nÄ± Ã§Ä±kar"""
        # Ä°ÅŸletme yanÄ±tÄ± seÃ§icileri
        reply_selectors = [
            "[class*='reply']",
            "[class*='response']",
            "[class*='owner']",
            "[class*='business-comment']",
            ".spoiler-view__reply"
        ]
        
        for selector in reply_selectors:
            try:
                reply_element = await review_element.query_selector(selector)
                if reply_element:
                    reply_text = await reply_element.text_content()
                    reply_text = reply_text.strip()
                    
                    # Gereksiz metinleri temizle
                    reply_text = re.sub(r'(Ä°ÅŸletme yanÄ±tÄ±|Business reply|Owner response|Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†Ğ°)[:\s]*', '', reply_text, flags=re.IGNORECASE)
                    reply_text = reply_text.strip()
                    
                    if reply_text and len(reply_text) > 5:
                        return reply_text
            except:
                pass
        
        return None
    
    async def scrape_all_reviews(self, business_url, max_reviews=None):
        """TÃ¼m yorumlarÄ± Ã§ek"""
        
        # Browser baÅŸlat
        await self.start_browser()
        
        try:
            # Ä°ÅŸletme sayfasÄ±na git ve bilgileri al
            business_id, business_name = await self.navigate_to_place(business_url)
            
            # Ä°ÅŸ yeri bilgilerini kaydet (otomatik kaydetme iÃ§in)
            self.business_id = business_id
            self.business_name = business_name
            
            if not business_id:
                logger.error("âŒ Ä°ÅŸletme bilgileri alÄ±namadÄ±!")
                return {
                    'business_id': None,
                    'business_name': None,
                    'reviews': [],
                    'scrape_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
            # YorumlarÄ± Ã§ek
            reviews = await self.scrape_reviews_with_continuous_scroll(max_reviews)
            
            # SonuÃ§larÄ± dÃ¶ndÃ¼r
            return {
                'business_id': business_id,
                'business_name': business_name,
                'reviews': reviews,
                'total_review_count': self.total_reviews,
                'scraped_review_count': len(reviews),
                'scrape_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'scrape_url': business_url
            }
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Scraping iÅŸlemi sÄ±rasÄ±nda hata: {e}")
            return {
                'business_id': None,
                'business_name': None,
                'reviews': [],
                'scrape_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'error': str(e)
            }
        
        finally:
            await self.close()
    
    async def save_to_files(self, data, filename_base):
        """Verileri dosyalara kaydet"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON kaydet
        json_filename = f"data/raw/{filename_base}_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # CSV kaydet
        if data and 'reviews' in data and data['reviews']:
            df = pd.DataFrame(data['reviews'])
            csv_filename = f"data/processed/{filename_base}_{timestamp}.csv"
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"ğŸ’¾ Veriler kaydedildi:")
            logger.info(f"   JSON: {json_filename}")
            logger.info(f"   CSV:  {csv_filename}")
            
            return json_filename, csv_filename
        else:
            logger.warning("âš ï¸ Kaydedilecek yorum verisi bulunamadÄ±")
            return None, None
    
    async def close(self):
        """Browser'Ä± kapat"""
        if self.browser:
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()

# Ana fonksiyon
async def main():
    """Ana scraping fonksiyonu"""
    
    # KullanÄ±cÄ± tercihlerini sor
    print("\nğŸš€ Yandex Maps GeliÅŸtirilmiÅŸ Yorum Scraper")
    print("=" * 50)
    print("Bu script, Yandex Maps yorumlarÄ±nÄ± yÃ¼ksek kalitede Ã§eker")
    
    # URL bilgisi
    default_url = "https://yandex.com.tr/maps/org/istanbul_havalimani/85454152633/?ll=28.752054%2C41.279299&utm_campaign=desktop&utm_medium=search&utm_source=maps&z=13.65"
    print(f"\nğŸ“ Hangi mekanÄ±n yorumlarÄ±nÄ± Ã§ekmek istiyorsunuz?")
    print(f"   VarsayÄ±lan: {default_url}")
    business_url = input("URL: ").strip() or default_url
    
    # Maksimum yorum sayÄ±sÄ±
    print("\nğŸ“Š Maksimum kaÃ§ yorum Ã§ekilsin?")
    print("   (VarsayÄ±lan: 2000, 'all' tÃ¼m yorumlar iÃ§in)")
    max_reviews_input = input("Maksimum yorum sayÄ±sÄ±: ").strip() or "2000"
    
    if max_reviews_input.lower() in ["all", "tÃ¼m", "hepsi"]:
        max_reviews = None  # TÃ¼m yorumlar
    else:
        try:
            max_reviews = int(max_reviews_input)
        except ValueError:
            print("âš ï¸ GeÃ§ersiz sayÄ±, varsayÄ±lan deÄŸer (2000) kullanÄ±lÄ±yor.")
            max_reviews = 2000
    
    # Headless mod ayarÄ±
    print("\nğŸ–¥ï¸ TarayÄ±cÄ± gÃ¶rÃ¼nÃ¼rlÃ¼ÄŸÃ¼:")
    print("   [1] GÃ¶rÃ¼nmez mod (daha hÄ±zlÄ±, arka planda Ã§alÄ±ÅŸÄ±r)")
    print("   [2] GÃ¶rÃ¼nÃ¼r mod (daha yavaÅŸ, tarayÄ±cÄ±yÄ± gÃ¶rebilirsiniz)")
    headless_choice = input("SeÃ§iminiz (1/2): ").strip() or "1"
    
    # Scraper'Ä± baÅŸlat
    scraper = YandexMapsScraper()
    
    try:
        # BaÅŸlangÄ±Ã§ zamanÄ±nÄ± kaydet
        start_time = time.time()
        
        # TarayÄ±cÄ± gÃ¶rÃ¼nÃ¼rlÃ¼k ayarÄ±nÄ± gÃ¼ncelle
        if headless_choice == "2":
            print("âœ… GÃ¶rÃ¼nÃ¼r mod seÃ§ildi, tarayÄ±cÄ± penceresi aÃ§Ä±lacak.")
            # start_browser fonksiyonu Ã§aÄŸrÄ±lmadan Ã¶nce browser nesnesini gÃ¼ncelle
            scraper.browser = None  # Ä°lk browser nesnesini temizle
            
        # YorumlarÄ± Ã§ek
        logger.info(f"ğŸš€ Yorum toplama iÅŸlemi baÅŸlatÄ±lÄ±yor: {business_url}")
        data = await scraper.scrape_all_reviews(
            business_url=business_url,
            max_reviews=max_reviews
        )
        
        # BitiÅŸ zamanÄ±nÄ± kaydet ve sÃ¼reyi hesapla
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        # Dosyalara kaydet
        await scraper.save_to_files(data, "yandex_reviews_enhanced")
        
        # Ã–zet bilgi
        if data and 'reviews' in data and data['reviews']:
            reviews = data['reviews']
            
            logger.info(f"\nğŸ“Š Ã–ZET:")
            logger.info(f"   Ä°ÅŸletme: {data['business_name']}")
            logger.info(f"   ID: {data['business_id']}")
            logger.info(f"   Sitede gÃ¶sterilen toplam yorum sayÄ±sÄ±: {data.get('total_review_count', 'Belirsiz')}")
            logger.info(f"   Ã‡ekilen yorum sayÄ±sÄ±: {len(reviews)}")
            logger.info(f"   Tekrar kontrolÃ¼nden geÃ§irilmiÅŸ veri")
            logger.info(f"   GeÃ§en sÃ¼re: {elapsed_time:.2f} saniye")
            
            valid_ratings = [r.get('rating') for r in reviews if r.get('rating') is not None]
            if valid_ratings:
                avg_rating = sum(valid_ratings) / len(valid_ratings)
                logger.info(f"   Ortalama puan: {avg_rating:.1f}â­")
            
            print("\n" + "=" * 40)
            print(f"âœ… Ä°ÅŸlem tamamlandÄ±!")
            print(f"ğŸ“Š Toplam {len(reviews)} yorum Ã§ekildi (sitede gÃ¶sterilen: {data.get('total_review_count', 'Belirsiz')})")
            print(f"â±ï¸ GeÃ§en sÃ¼re: {elapsed_time:.2f} saniye")
            print(f"ğŸ’¾ Veriler data/ klasÃ¶rÃ¼ne kaydedildi")
            print("=" * 40)
        else:
            logger.warning("âš ï¸ HiÃ§ yorum bulunamadÄ±!")
            print("\nâŒ HiÃ§ yorum bulunamadÄ±!")
    
    except Exception as e:
        logger.error(f"ğŸ’¥ Hata oluÅŸtu: {e}")
        print(f"\nâŒ Bir hata oluÅŸtu: {e}")
    
    finally:
        # Temizlik
        await scraper.close()
        print("\nğŸ‘‹ Yandex Maps Scraper kapatÄ±lÄ±yor...")

if __name__ == "__main__":
    # Scripti Ã§alÄ±ÅŸtÄ±r
    print("ğŸš€ Yandex Maps GeliÅŸtirilmiÅŸ Yorum Scraper baÅŸlatÄ±lÄ±yor...")
    asyncio.run(main())